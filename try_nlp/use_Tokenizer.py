# -*- coding: utf-8 -*-
# @Time    : 2023/9/12 14:46
# @Author  : XianYangğŸš€
# @Email   : xy_mts@163.com
# @File    : use_Tokenizer.py
# ------â¤ï¸â¤ï¸â¤ï¸------ #

from transformers import BertTokenizer

# åŠ è½½åˆ†è¯å·¥å…·
Tokenizer = BertTokenizer.from_pretrained("model/vocab.txt")
# print(token)

sents = ['ä½ç½®å°šå¯ï¼Œä½†è·ç¦»æµ·è¾¹çš„ä½ç½®æ¯”é¢„æœŸçš„è¦å·®çš„å¤š',
         '5æœˆ8æ—¥ä»˜æ¬¾æˆåŠŸï¼Œå½“å½“ç½‘æ˜¾ç¤º5æœˆ10æ—¥å‘è´§ï¼Œå¯æ˜¯è‡³ä»Šè¿˜æ²¡çœ‹åˆ°è´§ç‰©ï¼Œä¹Ÿæ²¡æ”¶åˆ°ä»»ä½•é€šçŸ¥ï¼Œç®€ä¸çŸ¥æ€ä¹ˆè¯´å¥½ï¼ï¼ï¼',
         'æ•´ä½“æ¥è¯´ï¼Œæœ¬ä¹¦è¿˜æ˜¯ä¸é”™çš„ã€‚è‡³å°‘åœ¨ä¹¦ä¸­æè¿°äº†è®¸å¤šç°å®ä¸­å­˜åœ¨çš„å¸æ³•ç³»ç»Ÿæ–¹é¢çš„é—®é¢˜ï¼Œè¿™æ˜¯å€¼å¾—æ¯ä¸ªæ³•å¾‹å·¥ä½œè€…å»æ€è€ƒçš„ã€‚å°¤å…¶æ˜¯è®©é‚£äº›æ¶‰ä¸–ä¸æ·±çš„æƒ³åŠ å…¥åˆ°å¾‹å¸ˆé˜Ÿä¼ä¸­çš„å¹´é’äººï¼Œçœ‹åˆ°äº†ç¤¾ä¼šç‰¹åˆ«æ˜¯ä¸­å›½å¸æ³•ç•ŒçœŸå®çš„ä¸€é¢ã€‚ç¼ºç‚¹æ˜¯ï¼šä¹¦ä¸­å¼•ç”¨äº†å¤§é‡çš„æ³•å¾‹æ¡æ–‡å’Œå¸æ³•è§£é‡Šï¼Œå¯¹äºå·²ç»æ˜¯å¾‹å¸ˆæˆ–æœ‰ä¸€å®šå·¥ä½œç»éªŒçš„æ³•å¾‹å·¥ä½œè€…æ¥è¯´æœ‰ç‚¹å¤šä½™ï¼Œè€Œä¸”æ‰€å çš„ç¯‡å¹…ä¸å°‘ï¼Œæœ‰å‡‘å­—æ•°çš„å«Œç–‘ã€‚æ•´ä½“æ¥è¯´è¿˜æ˜¯ä¸é”™çš„ã€‚ä¸è¦å¯¹ä¸€æœ¬ä¹¦æå¤ªé«˜çš„è¦æ±‚ã€‚']

# æ‰¹é‡ç¼–ç å¥å­
out = Tokenizer.batch_encode_plus(
    batch_text_or_text_pairs=[sents[0], sents[1]],
    add_special_tokens=True,
    # å½“å¥å­é•¿åº¦å¤§äºmax_lengthæ—¶ï¼Œæˆªæ–­
    truncation=True,
    max_length=30,
    # ä¸€å¾‹è¡¥0åˆ°max_lengthé•¿åº¦
    padding="max_length",
    # å¯å–å€¼ï¼štf,pt,np,é»˜è®¤ä¸ºè¿”å›list
    return_tensors=None,
    # è¿”å›attention_mask
    return_attention_mask=True,
    # è¿”å›token_type_ids
    return_token_type_ids=True,
    # è¿”å›special_tokens_mask ç‰¹æ®Šç¬¦å·æ ‡è¯†
    return_special_tokens_mask=True,
    # è¿”å›offsets_mapping æ ‡è¯†æ¯ä¸ªè¯çš„èµ·æ­¢ä½ç½®ï¼Œè¿™ä¸ªå‚æ•°åªèƒ½åœ¨BertTokenizerFastä½¿ç”¨
    # return_offsets_mapping=True,
    # è¿”å›lengthæ ‡è¯†é•¿åº¦
    return_length=True,
)
# print(out)
for k, v in out.items():
    print(k, ":", v)

print(Tokenizer.decode(out["input_ids"][0]), Tokenizer.decode(out["input_ids"][1]))

# è·å–å­—å…¸
vocab = Tokenizer.get_vocab()
print(type(vocab), len(vocab), "é˜³å…‰" in vocab)
# æ·»åŠ æ–°è¯
Tokenizer.add_tokens(new_tokens=["é˜³å…‰", "å¤§åœ°"])
# æ·»åŠ æ–°ç¬¦å·
Tokenizer.add_special_tokens({"eos_token": "[EOS]"})
vocab = Tokenizer.get_vocab()
print(type(vocab), len(vocab), "é˜³å…‰" in vocab, "[EOS]" in vocab)
# ç¼–ç æ–°å¥å­
out = Tokenizer.encode(
    text="é˜³å…‰æ´’åœ¨å¤§åœ°ä¸Š[EOS]",
    text_pair=None,
    truncation=True,
    padding="max_length",
    max_length=10,
    add_special_tokens=True,
    return_tensors=None
)
print(out)
print(Tokenizer.decode(out))
